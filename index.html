<!DOCTYPE html>
<html>

<head>
<meta name="viewport" content="width=device-width, initial-scale=1">
<style>
* {
  box-sizing: border-box;  
}

.column {
  float: left;
  width: 50%;
  padding: 10px;
}

.centercolumn{
  float: left;
  width: 25%;
  padding: 150px 0; 
}

.row:after {
  content: "";
  display: table;
  clear: both; 
}
</style>
</head>

<body>
<section>
  <div class="row">
    <div class="column">
      <img src="images/liming.jpg" 
      width="350" height="500" alt="Liming">
    </div>
    <div class="centercolumn">
      <center>
      <h1>Liming Wang</h1>
      <p>
      <b>Electrical and Computer Engineering<br>
      University of Illinois, Urbana-Champaign</b><br>
      <b>Email</b>: lwang114@illinois.edu <br>
      <a href="https://github.com/lwang114">Github</a>
      </p>
      </center>
    </div>
  </div>
  <p>
  <h3>Welcome!</h3>
  I'm a third-year PhD student, and a proud member of the UI Statistical Speech Technology group led by Prof. Mark Hasegawa-Johnson.
  Current research interest includes:
  <ul>
    <li> Nonparametric and hierarchical Bayesian inference
    <li> Natural language understanding 
    <li> Mathematical model of multimodal language acquisition
  </ul>
  </p>
</section>

<section>
  <h2>Publications</h2>
    <h3>Journal Paper</h3>
      <ul>
        <li> Liming Wang, Mark Hasegawa-Johnson. Multimodal Word Discovery with Spoken Descriptions
        and Visual Concepts. Transactions on Audio, Speech and Language Processing. <a href="https://github.com/lwang114/TASLP_2019">paper</a></li> 
      </ul>
    
    <h3>Conference Paper</h3>
      <ul>
        <li> Liming Wang, Shengyu Feng, Xudong Lin, Manling Li, Heng Ji and Shih-Fu Chang. Coreference by Appearance: VisuallyGrounded  Event  Coreference  Resolution. <i>The Fourth Workshop on Computational Models of Reference, Anaphora andCoreference (CRAC)</i>, 2021. <a href=https://www.researchgate.net/publication/355168520_Coreference_by_Appearance_Visually_Grounded_Event_Coreference_Resolution>paper</a></li> 
        <li> Liming Wang, Mark Hasegawa-Johnson. Align or Attend? Toward More Efficient and Accurate Spoken Word Discovery Using Speech-To-Image Retrieval. <i>International Conference
        on Acoustics, Speech and Signal Processing (ICASSP)</i>, 2021. <a href=https://www.researchgate.net/publication/345007040_ALIGN_OR_ATTEND_TOWARD_MORE_EFFICIENT_AND_ACCURATE_SPOKEN_WORD_DISCOVERY_USING_SPEECH-TO-IMAGE_RETRIEVAL>paper</a></li>
        <li> Liming Wang, Mark Hasegawa-Johnson. A DNN-HMM-DNN Hybrid Model for Discovering Word-like Units from Spoken Captions and Image Regions. <i>Interspeech 2020</i>. <a href="https://www.researchgate.net/publication/343218251_A_DNN-HMM-DNN_Hybrid_Model_for_Discovering_Word-like_Units_from_Spoken_Captions_and_Image_Regions">paper</a></li>
        <li> Liming Wang, Mark Hasegawa-Johnson. Multimodal Word Discovery with Phone Sequence and
        Image Concepts. <i>Interspeech 2019</i> (oral presentation). <a href="https://www.isca-speech.org/archive/Interspeech_2019/pdfs/1487.pdf">paper</a> <a href="docs/interspeech_2019_presentation-sept-18.pdf">presentation</a></li>
        <li> Graham Neubig et al., XNMT: The eXtensible Neural Machine Translation Toolkit. <i>Proceedings
        of the 13th Conference of the Association for Machine Translation in the America</i>, 2018. </li>
        <li> Odette Scharenborg et al., Linguistic Unit Discovery from Multimodal Inputs in Unwritten 
        Languages: Summary of the "Speaking Rosetta" JSALT 2017 Workshop. <i>International Conference
        on Acoustics, Speech and Signal Processing</i>, 2017.
      <ul>
</section>

<section>
  <h2>Projects and Codes</h2>
    <ul>
      <li> Multimodal word discovery: <a href="https://github.com/lwang114/MultimodalWordDiscovery">code</a></li>
      <li> Math word problem solver: <a href="https://github.com/wangxr14/Algebraic-Word-Problem-Solver">code and report</a></li>
      <li> Interior point method applied to SVM: <a href="codes/svm.py">Download_code</a> <a href="codes/dataset.csv">dataset</a></li>
      <li> Graphical Lasso: <a href="https://github.com/lwang114/GraphicalLasso/">code</a></li>
      <li> Regulatory network inference: <a href="https://github.com/lwang114/RegulatoryNetworkInference">code</a></li>
      <li> Visualization of spoken units with diffusion map: <!--<a href="https:"></li>-->
    <ul>
</section>

<section>
  <h2> Hobbies </h2>
  <ul>
    <li> Play tennis (member of the tennis club), jogging and juggling 
    <li> Play Hulusi, a traditional Chinese flute
    <li> My favourite podcasts: 
    <a href="https://www.sciencefriday.com">Science Friday</a>
    <a href="https://www.thetalkingmachines.com">Talking Machines</a>
  </ul>
</section>
</body>
</html>
